{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:15:43.229178Z",
     "iopub.status.busy": "2025-10-19T10:15:43.228737Z",
     "iopub.status.idle": "2025-10-19T10:15:43.234059Z",
     "shell.execute_reply": "2025-10-19T10:15:43.233172Z",
     "shell.execute_reply.started": "2025-10-19T10:15:43.229124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import rotate, shift, zoom\n",
    "\n",
    "def augment_mnist_image(\n",
    "    img,\n",
    "    rotate_range=15,\n",
    "    shift_range=2,\n",
    "    zoom_range=0.1,\n",
    "    noise_std=0.05,\n",
    "    flip_horizontal=False,\n",
    "    flip_vertical=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply random augmentations to a single MNIST image (28x28).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        Input 28×28 image (float32 or uint8)\n",
    "    rotate_range : float\n",
    "        Max rotation (± degrees)\n",
    "    shift_range : int\n",
    "        Max translation in pixels (for x and y)\n",
    "    zoom_range : float\n",
    "        Max zoom percentage (0.1 means between 0.9× and 1.1×)\n",
    "    noise_std : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    flip_horizontal : bool\n",
    "        Random horizontal flip\n",
    "    flip_vertical : bool\n",
    "        Random vertical flip\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Augmented 28×28 image\n",
    "    \"\"\"\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    # 1. random rotation\n",
    "    angle = np.random.uniform(-rotate_range, rotate_range)\n",
    "    img = rotate(img, angle, reshape=False, mode='nearest')\n",
    "\n",
    "    # 2. random shift\n",
    "    shift_x = np.random.uniform(-shift_range, shift_range)\n",
    "    shift_y = np.random.uniform(-shift_range, shift_range)\n",
    "    img = shift(img, shift=(shift_x, shift_y), mode='nearest')\n",
    "\n",
    "    # 3. random zoom\n",
    "    z = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n",
    "    zoomed = zoom(img, z)\n",
    "\n",
    "    # crop or pad to restore 28×28\n",
    "    if zoomed.shape[0] > 28:\n",
    "        start = (zoomed.shape[0] - 28) // 2\n",
    "        img = zoomed[start:start+28, start:start+28]\n",
    "    else:\n",
    "        pad = (28 - zoomed.shape[0]) // 2\n",
    "        img = np.pad(zoomed,\n",
    "                     ((pad, 28 - zoomed.shape[0] - pad),\n",
    "                      (pad, 28 - zoomed.shape[0] - pad)),\n",
    "                     mode='constant')\n",
    "\n",
    "    # 4. random flips\n",
    "    if flip_horizontal and np.random.rand() < 0.5:\n",
    "        img = np.fliplr(img)\n",
    "\n",
    "    if flip_vertical and np.random.rand() < 0.5:\n",
    "        img = np.flipud(img)\n",
    "\n",
    "    # 5. Gaussian noise\n",
    "    img += np.random.normal(0, noise_std, img.shape)\n",
    "\n",
    "    # clip to valid range\n",
    "    img = np.clip(img, 0, 255)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:15:45.076676Z",
     "iopub.status.busy": "2025-10-19T10:15:45.076412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_file = \"fii-nn-2025-homework-2/extended_mnist_train.pkl\"\n",
    "test_file = \"fii-nn-2025-homework-2/extended_mnist_test.pkl\"\n",
    "\n",
    "with open(train_file, \"rb\") as fp:\n",
    "    train = pickle.load(fp)\n",
    "\n",
    "with open(test_file, \"rb\") as fp:\n",
    "    test = pickle.load(fp)\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for image, label in train:\n",
    "    train_data.append(image.flatten())\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_data = []\n",
    "for image, label in test:\n",
    "    test_data.append(image.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data = []\n",
    "augmented_train_labels = []\n",
    "\n",
    "for img, label in zip(train_data, train_labels):\n",
    "    img_2d = img.reshape(28, 28)\n",
    "\n",
    "    # create 3 augmented versions\n",
    "    for _ in range(3):\n",
    "        aug = augment_mnist_image(img_2d)\n",
    "        augmented_train_data.append(aug.flatten())\n",
    "        augmented_train_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:45:44.026460Z",
     "iopub.status.busy": "2025-10-19T10:45:44.026166Z",
     "iopub.status.idle": "2025-10-19T10:45:44.031487Z",
     "shell.execute_reply": "2025-10-19T10:45:44.030512Z",
     "shell.execute_reply.started": "2025-10-19T10:45:44.026439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples:  60000\n",
      "Test samples:  10000\n",
      "Image shape:  (784,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shuffle_idx = np.random.permutation(len(train_data))\n",
    "train_data = np.array(train_data)[shuffle_idx]\n",
    "train_labels = np.array(train_labels)[shuffle_idx]\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "print(\"Train samples: \", len(train_data))\n",
    "print(\"Test samples: \", len(test_data))\n",
    "print(\"Image shape: \", train_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return data / 255.0\n",
    "\n",
    "train_data = np.array(normalize(train_data)).astype(np.float64)\n",
    "test_data = np.array(normalize(test_data)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(n_inputs, n_neurons):\n",
    "    return np.random.uniform(-np.sqrt(6 / (n_inputs + n_neurons)), np.sqrt(6 / (n_inputs + n_neurons)), (n_inputs, n_neurons)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape:  [(784, 100), (100, 10)]\n",
      "Biases shape:  [(100,), (10,)]\n"
     ]
    }
   ],
   "source": [
    "#Initialization\n",
    "n_out_neurons = 10\n",
    "n_hidden_neurons = 100\n",
    "learning_rate = 0.035\n",
    "step_size = 5\n",
    "batch_size = 32\n",
    "lambda_l1 = np.float64(0)\n",
    "lambda_l2 = np.float64(0.0001)\n",
    "dropout_p = 00\n",
    "epochs = 30\n",
    "momentum = 0.9  \n",
    "n_inputs = train_data.shape[1]\n",
    "weights = [xavier_init(n_inputs, n_hidden_neurons), xavier_init(n_hidden_neurons, n_out_neurons)]  # xavier initialization\n",
    "biases = [np.zeros(n_hidden_neurons, dtype=np.float64), np.zeros(n_out_neurons, dtype=np.float64)]\n",
    "momentum_w = [np.zeros_like(weights[0]), np.zeros_like(weights[1])] \n",
    "momentum_b = [np.zeros_like(biases[0]), np.zeros_like(biases[1])] \n",
    "\n",
    "print(\"Weights shape: \", [w.shape for w in weights])\n",
    "print(\"Biases shape: \", [b.shape for b in biases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability improvement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_test(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.clip(x, min = 0)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(batch, output_size):\n",
    "    result = np.zeros((len(batch), output_size))\n",
    "    for i, l in enumerate(batch):\n",
    "        result[i, l] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(target, y):\n",
    "    return -np.sum(target * np.log(y + 1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, batch_size):\n",
    "    return np.array_split(data, len(data) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w, b, training = True): \n",
    "    # (32, 784) @ (748, 100) = (32, 100)\n",
    "    # (32, 100) @ (100, 10) = (32, 10)\n",
    "    x_in = x\n",
    "    layers = len(w) \n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    y_list.append(x)\n",
    "\n",
    "    #ReLU on the hidden layers\n",
    "    for layer in range(layers - 1): \n",
    "        z = x_in @ w[layer] + b[layer] \n",
    "        z_list.append(z)\n",
    "\n",
    "        y = relu(z)\n",
    "\n",
    "        if training:\n",
    "            mask = np.random.binomial(size = len(y[1]), n = 1, p=(1 - dropout_p))\n",
    "            y = y * mask * (1 / 1 - dropout_p)  \n",
    "    \n",
    "        y_list.append(y) \n",
    "\n",
    "        x_in = y \n",
    "\n",
    "    # Softmax on the last layer (output layer)    \n",
    "    z = x_in @ w[-1] + b[-1]\n",
    "    z_list.append(z) \n",
    "\n",
    "    y = softmax(z) \n",
    "    y_list.append(y) \n",
    "\n",
    "    return (y_list, z_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_l2_penalty(grad_w, weights, l1_lambda, l2_lambda):\n",
    "    grad_w += l1_lambda * np.sign(weights) / batch_size + l2_lambda * weights / batch_size\n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y_list, z_list, target, w, b): \n",
    "    global momentum_w, momentum_b, momentum\n",
    "    L = len(w) \n",
    "    # softmax on the final layer \n",
    "    delta = y_list[-1] - target \n",
    "    gradient_w = y_list[-2].T @ delta \n",
    "    gradient_b = np.sum(delta, axis=0) \n",
    "\n",
    "    # Momentum update for last layer\n",
    "    momentum_w[L - 1] = momentum * momentum_w[L - 1] - (gradient_w * learning_rate / batch_size)\n",
    "    momentum_b[L - 1] = momentum * momentum_b[L - 1] - (gradient_b * learning_rate / batch_size)\n",
    "    w[L - 1] += momentum_w[L - 1]\n",
    "    b[L - 1] += momentum_b[L - 1]\n",
    "\n",
    "    for i in reversed(range(0, L - 1)): \n",
    "        delta = (delta @ w[i+1].T) * relu_deriv(z_list[i]) \n",
    "\n",
    "        gradient_w = y_list[i].T @ delta \n",
    "        gradient_b = np.sum(delta, axis = 0) \n",
    "\n",
    "        # L1, L2 reg\n",
    "        gradient_w += lambda_l1 * np.sign(w[i]) / batch_size + lambda_l2 * w[i] / batch_size\n",
    "        \n",
    "        # Momentum \n",
    "        momentum_w[i] = momentum * momentum_w[i] - (gradient_w * learning_rate / batch_size)\n",
    "        momentum_b[i] = momentum * momentum_b[i] - (gradient_b * learning_rate / batch_size)\n",
    "\n",
    "        # Update\n",
    "        w[i] += momentum_w[i]\n",
    "        b[i] += momentum_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape:  (32, 784)\n",
      "Batch label shape:  (32, 10)\n",
      "Weights shape:  [(784, 100), (100, 10)]\n",
      "Biases shape:  [(100,), (10,)]\n",
      "Starting training...\n",
      "Epoch 1 completed. Loss: 1.5633, Test Accuracy: 0.9602\n",
      "Epoch 2 completed. Loss: 1.5081, Test Accuracy: 0.9678\n",
      "Epoch 3 completed. Loss: 1.4953, Test Accuracy: 0.9684\n",
      "Epoch 4 completed. Loss: 1.4877, Test Accuracy: 0.9720\n",
      "Epoch 5 completed. Loss: 1.4827, Test Accuracy: 0.9734\n",
      "Epoch 6 completed. Loss: 1.4785, Test Accuracy: 0.9764\n",
      "Epoch 7 completed. Loss: 1.4754, Test Accuracy: 0.9740\n",
      "Epoch 8 completed. Loss: 1.4736, Test Accuracy: 0.9720\n",
      "Epoch 9 completed. Loss: 1.4721, Test Accuracy: 0.9737\n",
      "Epoch 10 completed. Loss: 1.4704, Test Accuracy: 0.9772\n",
      "Epoch 11 completed. Loss: 1.4693, Test Accuracy: 0.9773\n",
      "Epoch 12 completed. Loss: 1.4686, Test Accuracy: 0.9760\n",
      "Epoch 13 completed. Loss: 1.4676, Test Accuracy: 0.9761\n",
      "Epoch 14 completed. Loss: 1.4678, Test Accuracy: 0.9758\n",
      "Epoch 15 completed. Loss: 1.4664, Test Accuracy: 0.9771\n",
      "Epoch 16 completed. Loss: 1.4655, Test Accuracy: 0.9765\n",
      "Epoch 17 completed. Loss: 1.4649, Test Accuracy: 0.9776\n",
      "Epoch 18 completed. Loss: 1.4643, Test Accuracy: 0.9779\n",
      "Epoch 19 completed. Loss: 1.4639, Test Accuracy: 0.9784\n",
      "Epoch 20 completed. Loss: 1.4638, Test Accuracy: 0.9799\n",
      "Epoch 21 completed. Loss: 1.4626, Test Accuracy: 0.9796\n",
      "Epoch 22 completed. Loss: 1.4622, Test Accuracy: 0.9800\n",
      "Epoch 23 completed. Loss: 1.4618, Test Accuracy: 0.9794\n",
      "Epoch 24 completed. Loss: 1.4616, Test Accuracy: 0.9803\n",
      "Epoch 25 completed. Loss: 1.4615, Test Accuracy: 0.9808\n",
      "Epoch 26 completed. Loss: 1.4614, Test Accuracy: 0.9810\n",
      "Epoch 27 completed. Loss: 1.4614, Test Accuracy: 0.9809\n",
      "Epoch 28 completed. Loss: 1.4614, Test Accuracy: 0.9811\n",
      "Epoch 29 completed. Loss: 1.4614, Test Accuracy: 0.9810\n",
      "Epoch 30 completed. Loss: 1.4613, Test Accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "# Batch Training\n",
    "batches = split(train_data, batch_size)\n",
    "label_batches = split(train_labels, batch_size)\n",
    "label_batches = [one_hot(batch, n_out_neurons) for batch in label_batches]\n",
    "print(\"Batch shape: \", batches[0].shape)\n",
    "print(\"Batch label shape: \", label_batches[0].shape)\n",
    "print(\"Weights shape: \", [w.shape for w in weights])\n",
    "print(\"Biases shape: \", [b.shape for b in biases])\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = np.float64(0.0)\n",
    "\n",
    "    for x, target in zip(batches, label_batches):\n",
    "        (y_list, z_list) = forward(x, weights, biases)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(input = torch.tensor(y_list[-1]), target=torch.tensor(target))\n",
    "        epoch_loss += loss\n",
    "\n",
    "        backward(y_list, z_list, target, weights, biases)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_y_list, _ = forward(test_data, weights, biases, training = False)\n",
    "    test_predictions = np.argmax(test_y_list[-1], axis=1)\n",
    "    test_labels = np.array([label for _, label in test])\n",
    "    test_accuracy = np.mean(test_predictions == test_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss / len(batches):.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(test_data, weights, biases):\n",
    "    preds = []\n",
    "    for x in test_data:\n",
    "        # batchify the sample so forward() works correctly\n",
    "        x = x.reshape(1, -1)\n",
    "\n",
    "        y_list, _ = forward(x, weights, biases, training=False)\n",
    "        output = y_list[-1]           # softmax output (1, num_classes)\n",
    "        preds.append(np.argmax(output))\n",
    "    \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_csv = {\n",
    "    \"ID\": [],\n",
    "    \"target\": [],\n",
    "}\n",
    "\n",
    "predictions = test_network(test_data, weights, biases)\n",
    "\n",
    "for i, label in enumerate(predictions):\n",
    "    predictions_csv[\"ID\"].append(i)\n",
    "    predictions_csv[\"target\"].append(label)\n",
    "\n",
    "df = pd.DataFrame(predictions_csv)\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13980009,
     "sourceId": 115341,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
